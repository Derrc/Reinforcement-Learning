# TODO: Run more trials (current results based off 1 trial)

Control Hyperparameters:
- Train on 2000 Episodes
- Evaluation on 30 Episodes
- Optimizer = SGD
- Discount Factor = 0.99
- Learning Rate = 0.001
- Train Seed = 11
- Test Seed = 20
- Game Solved Score = 195 (mean from last 100 episodes)

SARSA:
- Using decaying epsilon greedy (1 / episode)
- Solved after just 659 Episodes
- Mean after evaluation: 509


Q-Learning:
- Using decaying epsilon greedy (1 / episode)
- Solved after just 804 Episodes
- Mean after evaluation: 109
- Problems:
  - samples aren't independent and identically distributed, they are sequential from environment interactions
  - sees too many of the same samples (same behavior is reinforced when acting greedily)
  - network loses ability to generalize and overfits the training environment (seed)
- Solutions:
  - experience replay
  - early stopping
  - decay learning rate


Deep Q-Learning Using DQN with Experience Replay:
- Using decaying epsilon greedy -> y = 0.01 + 0.99e^(-x/EPSILON_DECAY)
- Solved after
- Mean after evaluation:
- Problems:
  - subject to catastrophic forgetting
  - good explanation here: https://ai.stackexchange.com/questions/23810/why-do-my-rewards-reduce-after-extensive-training-using-d3qn





Research More:
- Off-Policy vs On-Policy Advantages/Disadvantages
- Overfitting in RL
  - decaying learning rate (Robbins-Munro sequence)
- Seed Selections
- Catastrophic Forgetting


Papers:
- Playing Atari with Deep Reinforcement Learning (DQN with Experience Replay)
  - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf
