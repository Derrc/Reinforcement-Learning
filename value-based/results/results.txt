Control Hyperparameters:
- Train on 2000 Episodes
- Evaluation on 30 Episodes
- Optimizer = SGD
- Discount Factor = 0.99
- Learning Rate = 0.001
- Seed = 11
- Game Solved Score = 195 (mean from last 100 episodes)

Q-Learning:
- Using decaying epsilon greedy (1 / episode)
- Solved after just 804 Episodes
- Mean after evaluation: 109
- Short term memory problem:
  - samples aren't independent and identically distributed, they are sequential from environment interactions
  - sees too many of the same samples (especially since model is acting greedily)
  - forgets old samples (when it reaches these samples again, reward plummets because agent doesn't remember what to do)