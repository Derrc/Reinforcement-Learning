# TODO: Run trials over more complex environments

Control Hyperparameters:
- 5 Trials
- Train on 2000 Episodes
- Evaluation on 30 Episodes
- Optimizer = SGD
- Discount Factor = 0.99
- Learning Rate = 0.001
- Epsilon Decay = 500
- Training Seed = 11
- Game Solved Score = 195 (mean from last 100 episodes)


Experiments over 5 Trials:
- Using decaying epsilon greedy -> y = 0.01 + 0.99e^(-x/EPSILON_DECAY)
  - better convergence and data distribution when allowing more exploration

SARSA:
- Mean Solved Episode over Trials: 1190.2
- Mean Reward over Trials: 76.0
- Mean Q-Value over Trials: 0.904

Q-Learning:
- Mean Reward over Trials: 62.0
- Mean Q-Value over Trials: 0.565
- Mean Solved Episode over Trials: 1344.6
- Problems:
  - samples aren't independent and identically distributed, they are sequential from environment interactions
  - sees too many of the same samples (same behavior is reinforced when acting greedily)
  - network loses ability to generalize and overfits the training environment
- Solutions:
  - experience replay
  - early stopping
  - decay learning rate


Deep Q-Learning Using DQN with Experience Replay:
- Never solved
- Mean Reward over Trials: 52.0
- Mean Q-Value over Trials: 2.873
- Problems:
  - unstable learning
  - overestimation of q-values
  - subject to catastrophic forgetting
  - good explanation here: https://ai.stackexchange.com/questions/23810/why-do-my-rewards-reduce-after-extensive-training-using-d3qn
  - add target network


Deep Q-Learning Using DQN with Experience Replay and Target Network:
- Mean Reward over Trials: 151.4
- Mean Q-Value over Trials: 44.189
- Mean Solved Episode over Trials: 459.4
- Performed dramatically better over other algorithms and just using experience replay
- Problems:
  - still could be overestimating q-values



Double DQN:
- proposed to solve overestimation of q-values







Research More:
- Off-Policy vs On-Policy Advantages/Disadvantages
- Overfitting in RL
  - decaying learning rate (Robbins-Munro sequence)
- Seed Selections
- Catastrophic Forgetting


Papers:
- Playing Atari with Deep Reinforcement Learning (DQN with Experience Replay)
  - https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf
