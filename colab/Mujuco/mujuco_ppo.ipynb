{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNORMsIW0gEW6tJwbnHw9am"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **PPO on Mujuco Environments**"],"metadata":{"id":"m6n6eUmtfm_R"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"80EbULkyLeOm"},"outputs":[],"source":["!apt-get install -y \\\n","    libgl1-mesa-dev \\\n","    libgl1-mesa-glx \\\n","    libglew-dev \\\n","    libosmesa6-dev \\\n","    software-properties-common\n","\n","!apt-get install -y patchelf"]},{"cell_type":"code","source":["!pip install gymnasium\n","!pip install free-mujoco-py"],"metadata":{"id":"eeRP2XaDfZH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import mujoco_py\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","from torch.distributions import Normal\n","import numpy as np\n","import random\n","import os\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","\n","# mount google drive for saving checkpoints\n","drive.mount('/content/gdrive')"],"metadata":{"id":"iUI5u75Pfb7K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n","    torch.nn.init.orthogonal_(layer.weight, std)\n","    torch.nn.init.constant_(layer.bias, bias_const)\n","    return layer\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_dim):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            layer_init(nn.Linear(state_dim, 128)),\n","            nn.ReLU(),    \n","            layer_init(nn.Linear(128, 1), std=1.0)\n","        )\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return x\n","\n","\n","class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim, action_low, action_high):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            layer_init(nn.Linear(state_dim, 64)),\n","            nn.Tanh(),    \n","            layer_init(nn.Linear(64, 64)),  \n","            nn.Tanh(),      \n","            layer_init(nn.Linear(64, action_dim), std=0.01)\n","        )\n","        self.action_dim = action_dim\n","        self.action_low = action_low\n","        self.action_high = action_high\n","        self.logstd = nn.Parameter(torch.zeros(action_dim))\n","\n","    def forward(self, x): \n","        mu = self.layers(x)\n","        std = torch.exp(self.logstd)\n","        # mean, std\n","        return mu, std\n","\n","    def get_log_probs(self, state, actions):\n","        mu, std = self.forward(state)\n","        dist = Normal(mu, std)\n","        log_prob = dist.log_prob(actions)\n","        entropy = dist.entropy()\n","\n","        return log_prob.sum(1), entropy.sum(1)\n","\n","    def get_action(self,state):\n","        mu, std = self.forward(state)\n","        dist = Normal(mu, std)\n","        action = torch.clamp(dist.sample(), self.action_low, self.action_high)\n","\n","        log_prob_action = dist.log_prob(action)\n","        return action, log_prob_action.sum(0)"],"metadata":{"id":"gmaVlkN6h5x8","executionInfo":{"status":"ok","timestamp":1671817958403,"user_tz":480,"elapsed":157,"user":{"displayName":"Derrick Kim","userId":"07385429809460231887"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["class PPOAgent():\n","    def __init__(self, env_name, record_video=False, **hyperparameters):\n","        self.make_env(env_name, record_video)\n","        self.obs_dim = self.env.observation_space.shape[0]\n","        self.action_dim = self.env.action_space.shape[0]\n","        self.action_low = self.env.action_space.low[0]\n","        self.action_high = self.env.action_space.high[0]\n","\n","        self.init_hyperparameters(hyperparameters)\n","\n","        self.actor = Actor(self.obs_dim, self.action_dim, self.action_low, self.action_high).to(self.device)\n","        self.critic = Critic(self.obs_dim).to(self.device)\n","\n","        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=self.actor_lr, eps=1e-6)\n","        self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n","\n","        self.actor_path = f'/content/gdrive/My Drive/RL/Mujuco/models/{env_name}_ppo_actor.pth'\n","        self.critic_path = f'/content/gdrive/My Drive/RL/Mujuco/models/{env_name}_ppo_critic.pth'\n","\n","    def make_env(self, env_name, record_video):\n","        self.env = gym.make(env_name)\n","        # currently not working\n","        if record_video:\n","            self.env = gym.wrappers.RecordVideo(\n","                self.env, \n","                f\"/content/gdrive/My Drive/RL/Mujuco/videos/{env_name}\",\n","            )\n","\n","    def init_hyperparameters(self, hyperparameters):\n","        # hyperparameters taken from PPO paper for Mujuco Envs\n","        self.total_steps = 2000000\n","        self.batch_size = 2048\n","        self.minibatch_size = 64\n","        self.discount = 0.99\n","        self.entropy_coef = 0.01\n","        self.policy_clip = 0.2\n","        self.kl_target = 0.01\n","        self.max_grad_norm = 0.5\n","        self.gae_param = 0.95\n","        self.optim_epochs = 10\n","        self.actor_lr = 3e-4\n","        self.critic_lr = 3e-4\n","        self.seed = None\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        for param, val in hyperparameters.items():\n","            exec('self.' + param + ' = ' + str(val))\n","\n","        # set seeds\n","        if self.seed != None:\n","            torch.manual_seed(self.seed)\n","            np.random.seed(self.seed)\n","            random.seed(self.seed)\n","            torch.cuda.manual_seed(self.seed)\n","            torch.backends.cudnn.deterministic = True\n","\n","    def load(self):\n","        self.actor.load_state_dict(torch.load(self.actor_path))\n","        self.critic.load_state_dict(torch.load(self.critic_path))\n","\n","    def save(self):\n","        torch.save(self.actor.state_dict(), self.actor_path)\n","        torch.save(self.critic.state_dict(), self.critic_path)\n"],"metadata":{"id":"AZHgwaD3n5t2","executionInfo":{"status":"ok","timestamp":1671821287968,"user_tz":480,"elapsed":136,"user":{"displayName":"Derrick Kim","userId":"07385429809460231887"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["def gae(values, next_values, rewards, dones, discount, gae_param, device):\n","    advantages = torch.zeros(len(rewards) + 1).to(device)\n","\n","    for i in reversed(range(len(rewards)-1)):\n","        nonterminal = 1 - dones[i]\n","        delta = rewards[i] + discount * next_values[i] - values[i]\n","        advantages[i] = delta + (nonterminal * discount * gae_param * advantages[i+1])\n","\n","    advantages = advantages[:len(rewards)]\n","    return advantages\n","\n","def clipped_loss(ratio, advantages, policy_clip):\n","    # no_clip = torch.clamp(ratio * advantages, min=1e3, max=None)\n","    no_clip = ratio * advantages\n","    clip = torch.clamp(ratio, 1 - policy_clip, 1 + policy_clip) * advantages\n","    return torch.min(no_clip, clip).mean()\n","\n","\n","def plot(env_name, global_step, rewards, entropy_loss, policy_loss):\n","    plt.figure(figsize=(20,5))\n","    plt.subplot(131)\n","    plt.plot(rewards)\n","    plt.title(f'Rewards at Step {global_step}')\n","    plt.subplot(132)\n","    plt.plot(entropy_loss)\n","    plt.title(f'Entropy Loss at Step {global_step}')\n","    plt.subplot(133)\n","    plt.plot(policy_loss)\n","    plt.title(f'Policy Loss at Step {global_step}')\n","    plt.savefig(f'/content/gdrive/My Drive/RL/Mujuco/results/{env_name}.png')\n","    plt.show()\n"],"metadata":{"id":"eLqCH_VlZrUD","executionInfo":{"status":"ok","timestamp":1671821286870,"user_tz":480,"elapsed":122,"user":{"displayName":"Derrick Kim","userId":"07385429809460231887"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def train(env_name, current_update, seed):\n","    agent = PPOAgent(env_name, record_video=False, seed=seed)\n","\n","    b_size = agent.batch_size\n","    device = agent.device\n","    env = agent.env\n","\n","    # load models if exists\n","    if os.path.exists(agent.actor_path):\n","        agent.load()\n","\n","    # storage tensors\n","    obs = torch.zeros((b_size, agent.obs_dim)).to(device)\n","    next_obs = torch.zeros((b_size, agent.obs_dim)).to(device)\n","    actions = torch.zeros((b_size, agent.action_dim)).to(device)\n","    log_probs = torch.zeros((b_size)).to(device)\n","    rewards = torch.zeros(b_size).to(device)\n","    dones = torch.zeros(b_size).to(device)\n","\n","    global_step = 0\n","    total_rewards, total_entropy, total_loss = [], [], []\n","    num_updates = agent.total_steps // b_size\n","    num_minibatches = b_size // agent.minibatch_size\n","    ob = torch.tensor(env.reset(seed=seed)[0], dtype=torch.float32).to(device)\n","\n","    for update in range(current_update, num_updates):\n","        # stop at 1 mil steps\n","        if global_step > 1e6:\n","            break\n","\n","        # anneal lr of optimizers\n","        rate = 1.0 - (update - 1.0) / num_updates\n","        lr = rate * agent.actor_lr\n","        agent.actor_optim.param_groups[0]['lr'] = lr\n","        agent.critic_optim.param_groups[0]['lr'] = lr\n","\n","        # run trajectories -> generate batch\n","        for step in range(b_size):\n","            global_step += 1\n","\n","            with torch.no_grad():\n","                action, log_prob = agent.actor.get_action(ob)\n","\n","            next_ob, reward, terminated, truncated, info = env.step(action.cpu())\n","            next_ob = torch.tensor(next_ob, dtype=torch.float32).to(device)\n","\n","            obs[step] = ob\n","            next_obs[step] = next_ob\n","            actions[step] = action\n","            log_probs[step] = log_prob\n","            rewards[step] = reward\n","            dones[step] = torch.tensor(np.logical_or(terminated, truncated), dtype=torch.float32)\n","\n","            ob = next_ob.clone()\n","            if terminated or truncated:\n","                ob = torch.tensor(env.reset(seed=seed)[0], dtype=torch.float32).to(device)\n","\n","        # estimate returns and advantages using GAE\n","        with torch.no_grad():\n","            values = agent.critic(obs)\n","            next_values = agent.critic(next_obs)\n","            advantages = gae(values, next_values, rewards, dones, agent.discount, agent.gae_param, device)\n","            returns = advantages + values.squeeze(1)\n","\n","        # update policy and value function using mini-batches\n","        mb_size = agent.minibatch_size\n","        b_indices = np.arange(b_size)\n","        for step in range(agent.optim_epochs):\n","            np.random.shuffle(b_indices)\n","            for i in range(num_minibatches):\n","                start, end = mb_size * i, mb_size * (i+1) # inclusive, non-inclusive\n","                mb_indices = b_indices[start:end]\n","                mb_obs = obs[mb_indices]\n","                mb_actions = actions[mb_indices]\n","                mb_lps = log_probs[mb_indices]\n","                mb_returns = returns[mb_indices]\n","                mb_advantages = advantages[mb_indices]\n","                # normalize advantages every mini-batch\n","                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","                # with torch.autograd.detect_anomaly(check_nan=True):\n","                new_lps, entropy = agent.actor.get_log_probs(mb_obs, mb_actions)\n","                new_values = agent.critic(mb_obs)\n","                log_ratio = new_lps - mb_lps\n","                # FIX: clamp log_ratio so it can't get too big or small otherwise crash at 200k steps\n","                log_ratio = torch.clamp(log_ratio, -10, 1)\n","                ratio = torch.exp(log_ratio)\n","\n","                # implement value clipping\n","                value_loss = 0.5 * (new_values - mb_returns).pow(2).mean()\n","                agent.critic_optim.zero_grad()\n","                value_loss.backward()\n","                nn.utils.clip_grad_norm_(agent.critic.parameters(), agent.max_grad_norm)\n","                agent.critic_optim.step()\n","\n","                entropy_loss = entropy.mean()\n","\n","                policy_loss = -clipped_loss(ratio, mb_advantages, agent.policy_clip)\n","                agent.actor_optim.zero_grad()\n","                policy_loss.backward()\n","                nn.utils.clip_grad_norm_(agent.actor.parameters(), agent.max_grad_norm)\n","                agent.actor_optim.step()\n","\n","                # for plotting\n","                total_rewards.append(np.sum(rewards.cpu().numpy()))\n","                total_entropy.append(entropy_loss.detach().cpu().numpy())\n","                total_loss.append(policy_loss.detach().cpu().numpy())\n","\n","\n","        if update % 10 == 0:\n","            plot(env_name, global_step, total_rewards, total_entropy, total_loss)\n","            agent.save()"],"metadata":{"id":"cPoFR3kEyVBv","executionInfo":{"status":"ok","timestamp":1671823620453,"user_tz":480,"elapsed":164,"user":{"displayName":"Derrick Kim","userId":"07385429809460231887"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["# **Training**"],"metadata":{"id":"poCz4eYsntJJ"}},{"cell_type":"code","source":["# ENVS\n","MOUNTAIN_CAR = 'MountainCarContinuous-v0'\n","ANT = 'Ant-v2'\n","CHEETAH = 'HalfCheetah-v2'\n","HUMANOID = 'Humanoid-v2'\n","\n","current_update = 1\n","train(HUMANOID, current_update, seed=10)"],"metadata":{"id":"4ZVtvbmrjHV2"},"execution_count":null,"outputs":[]}]}